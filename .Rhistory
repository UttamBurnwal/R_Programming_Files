my_list2[[3]][2]
Fruit_list <- list(Apple = 85, Banana = 45, Guava = 100)
Fruit_list$Apple
#-----------------------------------------------
#Matrix
mat1<-matrix(c(1,2,3,4),nrow=2,byrow = T)
mat2<-matrix(c("a","b","c","d"),nrow=2,byrow = T)
mat3<-matrix(c(T,F,T,F),nrow=2,byrow = T)
mat1[1,]
mat1[,1]
mat1[2,1]
#transpose mat1
t(mat1)
mean(mat1[,2])
mean(mat1[2,])
#--------------------
# ARRAY
a1 <- array(1:24,dim = c(2,4,3))
a1[1,2,3]
a1[2,,1]
#-------------------------------------------------
#FACTOR
my_data<-c("Male","Female","Female","Male")
my_data <- as.factor(my_data)
#-------------------------------------------------
# DATAFRAME
df <- data.frame(Name=c("Sam","Bob"),Age=c(32,48),stringsAsFactors=F)
#strings as factors
customer_churn <- read.csv("/Users/abhishekdas/Desktop/Intellipat/Data_Science_Course_Documents/customer_churn.csv",stringsAsFactors = T)
#without strings as factors
customer_churn <- read.csv("/Users/abhishekdas/Desktop/Intellipat/Data_Science_Course_Documents/customer_churn.csv")
customer_churn <- read.csv("customer_churn.csv")
#selecting a column
customer_churn$gender
#column names
names(customer_churn)
#df <- data.frame(customer_churn[1:20,c(1,2,3,4)])
a <- customer_churn[,c(1,3,6)]
b <- customer_churn[,2:5]
d <- customer_churn[3,]
e <- customer_churn[c(3,5,7),]
f <- customer_churn[5:10,]
df1 <- customer_churn[4:8,2:5]
df2 <- customer_churn[50:60,c(2,3)]
df3 <- customer_churn[c(100:200,1000:2000),5:8]
df4 <- customer_churn[,c("gender","Partner")]
#----------------------------------------
#Decision Making
if(10>20){
print("10 is less than 20")
}
#------------
if(10<20){
print("10 is less than 20")
}
#-----------
if(10>20){
print("10 is less than 20")
}else{
print("10 is greater than 20")
}
#----------------------------------------------------
sum(is.na(customer_churn))
colSums(is.na(customer_churn))
# ways of imputing missing value
customer_churn$TotalCharges <- ifelse(is.na(customer_churn$TotalCharges),0,customer_churn$TotalCharges)
customer_churn$TotalCharges <- ifelse(is.na(customer_churn$TotalCharges),mean(customer_churn$TotalCharges,na.rm = T),customer_churn$TotalCharges)
customer_churn$TotalCharges[is.na(customer_churn$TotalCharges)] <- mean(customer_churn$TotalCharges, na.rm = TRUE)
df <- na.omit(customer_churn)
#-----------------------------------------------
#Looping
a<-1:9
for (i in a) {
print(i*2)
}
#-------------------------------------------------------
i=1
while (i<=10) {
print(i+2)
i<-i+1
}
# for(value in that) {
#   do this
# }
#----------------------------------------------------------
str(customer_churn)
head(customer_churn)
head(customer_churn,10)
head(customer_churn,2)
tail(customer_churn)
tail(customer_churn,3)
tail(customer_churn,10)
nrow(customer_churn)
ncol(customer_churn)
dim(customer_churn)
max(c(1,2,3,4,5))
max(customer_churn$MonthlyCharges)
min(c(1,2,3,4,5))
min(customer_churn$MonthlyCharges)
mean(c(1,2,3,4,5))
mean(customer_churn$MonthlyCharges)
#--------------------------------------------------------------
data.frame(Name=c("Sam","Bob"),Marks=c(97,25)) -> student
as.character(student$Name) -> student$Name
student <- rbind(student,c("Anne",75))
#-------------------------------------------------------------
data.frame(Name=c("Sam","Bob"),Marks=c(97,25)) -> student
as.character(student$Name) -> student$Name
student <- cbind(student,Grade=c("A","C"))
#-------------------------------------------------------------
#Merging Dataframes
data.frame(Department=c("Tech","Analytics","Support"),Location=c("Chicago","New York","Boston")) -> Employee
data.frame(Name=c("Sam","Bob","Anne"),Salary=c(75000,105000,120000),Department=c("Tech","Sales","Analytics")) -> Department2
merge(Employee,Department2,by="Department")
merge(Employee,Department2,by="Department",all = T)
merge(Employee,Department2,by="Department",all.x  = T)
merge(Employee,Department2,by="Department",all.y  = T)
#---------------------------------------------------
#Own defined functions
Add_five <- function(x){
x+5
}
Add_five(3)
Add_five(c(10,15,20))
#############################
#read data
#strings as factors = TRUE
customer_churn <- read.csv("D:/Data Science Course/R-Programming/Materials/R Programming Materials/Data Set/customer_churn.csv",stringsAsFactors = T)
#load libraraies
#install.packages(c("caTools","ModelMetrics"))
library(caTools)
library(ModelMetrics)
library(ggplot2)
#split the data
sample.split(customer_churn$MonthlyCharges,SplitRatio = 0.7)-> split_tag
subset(customer_churn, split_tag==T)->train
subset(customer_churn, split_tag==F)->test
#another way of splitting
rows = seq(1,nrow(customer_churn))
trainRows = sample(rows,(70*nrow(customer_churn))/100)
train1 = customer_churn[trainRows,]
test1 = customer_churn[-trainRows,]
#Linear regression model - training
#lm(Y ~ X, data=training_data)-> model1
lm(MonthlyCharges ~ tenure, data=train1)-> model1
#Summary of the model
summary(model1)
#predict on test
predict(model1, newdata=test1)->predicted_values
#data frame - test actuals and predicted ones
data.frame(Actual=test1$MonthlyCharges,Predicted=predicted_values)->final_data
#Residual
final_data$Actual - final_data$Predicted -> final_data$error
#RMSE
sqrt(mean((final_data$error)^2)) -> rmse1
rmse1
# Root Mean Squared Error
# It's the square root of the average of squared differences between actual and predicted values
# Since the errors are squared before they are averaged,
# the RMSE gives a relatively high weight to large errors.
# RMSE is useful when large errors are particularly undesirable.
rmse(final_data$Actual,final_data$Predicted)
#MAE
mae(final_data$Actual,final_data$Predicted)
# Mean Absolute Error
# all individual differences are weighted equally in the average
#---------------------------------------------------
#considering all variables other than MonthlyCharges as independent variables
train1$customerID <- NULL
test1$customerID <- NULL
lm(MonthlyCharges~tenure+PhoneService+InternetService,data=train1)-> model2
#summary
summary(model2)
predict(model2,newdata=test1)-> result
cbind(Actual=test1$MonthlyCharges,Predicted=result)->final_data2
as.data.frame(final_data2)-> final_data2
(final_data2$Actual-final_data2$Predicted)->error2
cbind(final_data2,error2) -> final_data2
sqrt(mean((final_data2$error2)^2))-> rmse2
rmse2
rmse(final_data2$Actual,final_data2$Predicted)
#MAE
mae(final_data2$Actual,final_data2$Predicted)
#-----------------------------------------------------------------------------------------------
#Multiple Linear Regression
sample.split(customer_churn$tenure,SplitRatio = 0.65)-> split_model
subset(customer_churn, split_model==T)->train
subset(customer_churn, split_model==F)->test
nrow(train)
nrow(test)
lm(tenure~MonthlyCharges+gender+InternetService+Contract, data=train)-> mod1
predict(mod1,test)->result1
cbind(Actual=test$tenure,Predicted=result1)->final_data1
head(final_data1)
class(final_data1)
as.data.frame(final_data1)->final_data1
class(final_data1)
final_data1$Actual - final_data1$Predicted ->error1
head(error1)
cbind(final_data1,error1)-> final_data1
head(final_data1)
sqrt(mean((final_data1$error1)^2))->rmse1
#-------------------------------------------------
lm(tenure~Partner+PhoneService+TotalCharges+PaymentMethod,data=train)-> mod2
predict(mod2,test)-> result2
cbind(Actual=test$tenure,Predicted=result2)->final_data2
head(final_data2)
as.data.frame(final_data2)-> final_data2
(final_data2$Actual-final_data2$Predicted)->error2
head(error2)
cbind(final_data2,error2) -> final_data2
sqrt(mean((final_data2$error2)^2,na.rm=T))->rmse2
#---------------------------------------------------
#Assumptions
ggplot(data= customer_churn, aes(x=tenure, y=TotalCharges)) + geom_point()
ggplot(data= customer_churn, aes(x=tenure, y=TotalCharges)) + geom_point()+geom_smooth(method = "lm")
lm(TotalCharges~tenure, data = customer_churn)->mod1
predict(mod1,data=customer_churn)-> result1
cbind(Actual=customer_churn$TotalCharges, Predicted=result1)-> final_data1
head(final_data1)
as.data.frame(final_data1)->final_data1
final_data1$Actual -final_data1$Predicted  -> error1
cbind(final_data1,error1)-> final_data1
ggplot(data= final_data1, aes(x=Predicted, y=error1)) + geom_point()
qqnorm(final_data1$error1)
#read data
#strings as factors = TRUE
customer_churn <- read.csv("D:/Data Science Course/R-Programming/Materials/R Programming Materials/Data Set/customer_churn.csv",stringsAsFactors = T)
library(caTools)
library(ROCR)
library(caret)
library(e1071)
library(ggplot2)
customer_churn$Churn_new <- ifelse(customer_churn$Churn == "Yes",1,0)
customer_churn$Churn_new <- as.factor(customer_churn$Churn_new)
#barplot
barplot(table(customer_churn$Churn_new))
table(customer_churn$Churn_new)
prop.table(table(customer_churn$Churn_new)) * 100
df <- data.frame(table(customer_churn$Churn))
ggplot(df,aes(x=Var1,y=Freq,fill=Var1)) + geom_bar(stat="identity")+
geom_text(aes(label=Freq),vjust=1.6) + xlab("Churn") + ggtitle("Churn Distribution")
#Split the data into train and test data sets
rows=seq(1,nrow(customer_churn))
trainRows=sample(rows,(70*nrow(customer_churn))/100)
train1 = customer_churn[trainRows,]
test1 = customer_churn[-trainRows,]
#logistic regression model
glm(Churn ~ MonthlyCharges, data= customer_churn, family="binomial") ->log_mod1
summary(log_mod1)
predict(log_mod1,data.frame(MonthlyCharges=50),type="response")
predict(log_mod1,data.frame(MonthlyCharges=77),type="response")
predict(log_mod1,data.frame(MonthlyCharges=20:100),type="response")
glm(Churn~tenure, data= customer_churn, family="binomial") ->log_mod2
summary(log_mod2)
predict(log_mod2,data.frame(tenure=10),type="response")
predict(log_mod2,data.frame(tenure=70),type="response")
predict(log_mod2,data.frame(tenure=10:70),type="response")
#----------------------------------------
#logistic regression model
glm(Churn_new ~ MonthlyCharges+Partner+InternetService, data=train1, family = "binomial")-> mod_log
#summary of the model
summary(mod_log)
#AIC (Akaike Information Criteria) â
#The analogous metric of adjusted RÂ² in logistic regression is AIC.
#AIC is the measure of fit which penalizes model for the number of model coefficients.
#Therefore, we always prefer model with minimum AIC value.
# Null Deviance and Residual Deviance â
# Null Deviance indicates the response predicted by a model with nothing but an intercept. Lower the value, better the model.
# Residual deviance indicates the response predicted by a model on adding independent variables. Lower the value, better the model.
#prediction on test
predict(mod_log, test1, type="response")->result_log
range(result_log)
table(test1$Churn_new, result_log>=0.4)
final_data <- data.frame(Actual=test1$Churn_new,Predicted=result_log)
final_data$predit_values <- ifelse(final_data$Predicted>=0.4,1,0)
final_data$predit_values <- as.factor(final_data$predit_values)
confusionMatrix(final_data$predit_values, final_data$Actual)
prediction(result_log,test1$Churn_new) -> predict_log
performance(predict_log,"acc")->acc
#plot(acc)
#table(test$Churn, result_log>0.4)
performance(predict_log,"tpr","fpr") -> roc_curve
plot(roc_curve)
plot(roc_curve, colorize=T)
#performance(predict_log,"auc")->auc
#auc
auc.tmp <- performance(predict_log,"auc"); auc <- as.numeric(auc.tmp@y.values)
auc
#Higher the area under curve, better the prediction power of the model.
#---------------------------------------------------------
#Multiple Logistic Regression
glm(Churn~gender+Partner+InternetService+MonthlyCharges, data=train, family = "binomial")-> mod_log1
predict(mod_log1,newdata=test,type="response")->result_log1
head(result_log1)
range(result_log1)
table(test$Churn, result_log1>0.4)
#--
glm(Churn~PaymentMethod+TechSupport+tenure+PaperlessBilling, data=train, family = "binomial")-> mod_log2
predict(mod_log2,newdata=test,type="response")->result_log2
head(result_log2)
range(result_log2)
table(test$Churn, result_log2>0.4)
#--
glm(Churn~Contract+Dependents+MultipleLines+DeviceProtection, data=train, family = "binomial")-> mod_log3
predict(mod_log3,newdata=test,type="response")->result_log3
head(result_log3)
range(result_log3)
table(test$Churn, result_log3>0.4)
#-----------------------------------------------------------------------------------------
#ROCR
sample.split(customer_churn$Churn,SplitRatio = 0.65)-> split_tag
subset(customer_churn, split_tag==T)->train
subset(customer_churn, split_tag==F)->test
nrow(train)
nrow(test)
glm(Churn~MonthlyCharges, data=train1, family = "binomial")-> mod_log
predict(mod_log,newdata=test,type="response")->result_log
head(result_log)
range(result_log)
table(test1$Churn, result_log>0.1)
prediction(result_log,test1$Churn) -> predict_log
performance(predict_log,"acc")->acc
plot(acc)
table(test$Churn, result_log>0.4)
performance(predict_log,"tpr","fpr") -> roc_curve
plot(roc_curve)
plot(roc_curve, colorize=T)
auc.tmp <- performance(predict_log,"auc"); auc <- as.numeric(auc.tmp@y.values)
auc
##############################################################
# Clear Global environment
rm(list=ls())
library(ggplot2)
library(dplyr)
library(splitstackshape)
library(corrplot)
#strings as factors = TRUE
customer_churn <- read.csv("D:/Data Science Course/R-Programming/Materials/R Programming Materials/Data Set/customer_churn.csv",stringsAsFactors = T)
#mean
mean(customer_churn$MonthlyCharges)
mean(customer_churn$TotalCharges,na.rm = T)
#median
median(customer_churn$MonthlyCharges)
# Mode
#method1
gmode <- function(v) {
uniqv <- unique(v)
uniqv[which.max(tabulate(match(v,uniqv)))]
}
#method2
# names function on the table returns the names of the values represented in the table.
# choose the name which corresponds to the value which has been counted most.
mymode <- function(vec){
mt <- table(vec)
names(mt)[mt == max(mt)]
}
xyz <- c(1,1,1,1,1,4,5,6,2,2,3,4,1,1,1,1,1)
mymode(xyz)
gmode(xyz)
gmode(customer_churn$PhoneService)
mymode(customer_churn$PhoneService)
#-----------------------------------------------------------------
#range
range(c(1,2,3,4,5))
range(customer_churn$MonthlyCharges)
# absolute frequencies
df1 <- data.frame(table(customer_churn$gender))
df2 <- data.frame(table(customer_churn$InternetService))
# relative frequencies
df3 <- data.frame(table(customer_churn$InternetService)/length(customer_churn$InternetService))
# relative frequencies in percent
(table(customer_churn$InternetService)/length(customer_churn$InternetService)) * 100
#2
prop.table(table(customer_churn$InternetService)) * 100
#barplot
barplot(table(customer_churn$gender))
barplot(table(customer_churn$gender)/length(customer_churn$gender))
#---------------------------------------------------------------
# Quantiles are a generalization of the idea of the median.
# The median is the value which splits the data into two equal parts.
# Similarly, a quantile partitions the data into other proportions.
# For example, a 25 %-quantile splits the data into two parts such that at
# least 25 % of the values are less than or equal to the quantile and at least
# 75 % of the values are greater than or equal to the quantile.
summary(customer_churn$tenure)
quantile(customer_churn$tenure)
quantile(customer_churn$tenure, probs=c(0,0.25,0.5,0.75,1))
#The interquartile range is defined as the difference between the 75th and 25th quartiles
# is the range of the middle 50% of the data
IQR(customer_churn$tenure)
#variance and standard deviation
var(customer_churn$tenure)
sd(customer_churn$tenure)
customer_churn$month <- 5
var(customer_churn$month)
sd(customer_churn$month)
#---------------------------------------------------------------
#sample
sample(1:100,3)
sample(customer_churn$customerID,5)
sample(customer_churn$customerID,20)
die <- 1:6
dice <- sample(die,size = 2, replace = T)
sum(dice)
roll <- function() {
die <- 1:6
dice <- sample(die,size = 2, replace = T)
sum(dice)
}
roll()
# Stratified Sampling -------------------------------------
df <- data.frame(table(customer_churn$Churn))
ggplot(df,aes(x=Var1,y=Freq,fill=Var1)) + geom_bar(stat="identity")+
geom_text(aes(label=Freq),vjust = 1.6)+
xlab("Churn") + ggtitle("Churn Distribution")
#using stratified function
sub_df <- stratified(customer_churn, "Churn", 1000)
sub_df2 <- stratified(customer_churn, "Churn", .1)
#using dplyr package
sub_df3 <- customer_churn %>%
group_by(Churn) %>%
sample_n(1000)
sub_df4 <- customer_churn %>%
group_by(Churn) %>%
sample_frac(.1)
#----------------------------------------------------------------
# Scaling and Normalization
#we scale our data before employing a distance based algorithm so that all the
#features contribute equally to the result.
# Standardization is a scaling technique where the values are centered around the mean with
# a unit standard deviation.
# This means that the mean of the attribute becomes zero and the resultant distribution
# has a unit standard deviation.
customer_churn$sc_mon <-  scale(customer_churn$MonthlyCharges)
mean(customer_churn$MonthlyCharges)
mean(customer_churn$sc_mon)
sd(customer_churn$MonthlyCharges)
sd(customer_churn$sc_mon)
#Normalization
#Normalization is a scaling technique in which values are shifted and rescaled so that they end up
#ranging between 0 and 1.
#It is also known as Min-Max scaling.
norml <- function(x) {
return ((x-min(x))/(max(x)-min(x)))
}
customer_churn$norm_mon <-  norml(customer_churn$MonthlyCharges)
#-----------------------------------------------------------------
?mtcars
# There are seven visualization methods (parameter method) in corrplot package,
# "circle", "square", "ellipse", "number", "shade", "color", "pie".
#
# Positive correlations are displayed in blue and negative correlations in red color.
# Color intensity and the size of the circle are proportional to the correlation coefficients.
#correlation matrix
M <- cor(mtcars)
corrplot(M, method = "circle")
corrplot(M, method = "square")
corrplot(M, method = "ellipse")
corrplot(M, method = "number") # Display the correlation coefficient
#corrplot.mixed() is a wrapped function for mixed visualization style.
corrplot.mixed(M)
corrplot.mixed(M, lower.col = "black", number.cex = .8)
# Layout
# There are three layout types (parameter type):
#
# "full" (default) : display full correlation matrix
# "upper" : display upper triangular of the correlation matrix
# "lower" : display lower triangular of the correlation matrix
corrplot(M, type = "upper")
corrplot(M, type = "lower")
# Clear Global environment
rm(list=ls())
customer_churn <- read.csv("D:/Data Science Course/R-Programming/Materials/R Programming Materials/Data Set/customer_churn.csv")
library(dplyr)
customer_churn %>% select("tenure","MonthlyCharges","TotalCharges")-> customer_features
head(customer_features)
kmeans(customer_features$MonthlyCharges,3)-> k_month
cbind(Month=customer_features$MonthlyCharges, Clusters=k_month$cluster) ->month_group
head(month_group)
as.data.frame(month_group)->month_group
month_group %>% filter(Clusters==1)-> month_group_1
month_group %>% filter(Clusters==2)-> month_group_2
month_group %>% filter(Clusters==3)-> month_group_3
head(month_group_1)
head(month_group_2)
head(month_group_3)
##############
kmeans(customer_features$tenure,3)->tenure_group
tenure_group
cbind(Tenure=customer_features$tenure, Clusters=tenure_group$cluster) ->tenure_group_data
head(tenure_group_data)
as.data.frame(tenure_group_data)->tenure_group_data
tenure_group_data %>% filter(Clusters==1)-> tenure_group_data1
tenure_group_data %>% filter(Clusters==2)-> tenure_group_data2
tenure_group_data %>% filter(Clusters==3)-> tenure_group_data3
head(tenure_group_data1)
head(tenure_group_data2)
head(tenure_group_data3)
#--
na.omit(customer_features)->customer_features
kmeans(customer_features$TotalCharges,3)->k_total
cbind(Total=customer_features$TotalCharges, Clusters=k_total$cluster) ->total_group
head(total_group)
as.data.frame(total_group)->total_group
total_group %>% filter(Clusters==1)-> total_group1
total_group %>% filter(Clusters==2)-> total_group2
total_group %>% filter(Clusters==3)-> total_group3
head(total_group1)
head(total_group2)
head(total_group3)
#######################################################
